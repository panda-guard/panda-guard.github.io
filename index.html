<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PANDAGUARD</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://floyedshen.github.io/" target="_blank">Guobin Shen</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=2E9Drq8AAAAJ&hl=en" target="_blank">Dongcheng Zhao</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a> Linghao Feng </a></sup>,
                  </span>
                  <span class="author-block">
                    <a> Xiang He </a></sup>,
                  </span>
                  <span class="author-block">
                    <a> Jihang Wang </a></sup>,
                  </span>
                  <span class="author-block">
                    <a> Sicheng Shen </a></sup>,
                  </span>
                  <span class="author-block">
                    <a> Haibo Tong </a></sup>,
                  </span>
                  <span class="author-block">
                    <a> Yiting Dong </a></sup>,
                  </span>
                  <span class="author-block">
                    <a> Jindong Li </a></sup>,
                  </span>
                  <span class="author-block">
                    <a> Xiang Zheng </a></sup>,
                  </span>
                  <span class="author-block">
                    <a> Yi Zeng </a>
                  </span>
                  </div>

<div class="is-size-5 publication-authors">
  <div class="author-block">Beijing Institute of AI Safety and Governance (Beijing-AISI)</div>
  <div class="author-block">Beijing Key Laboratory of Safe AI and Superalignment</div>
      <div style="height: 1px;"></div> <!-- 强制换行空DIV -->
  <div class="author-block">BrainCog Lab, CASIA</div>
    <div style="height: 1px;"></div>
  <div class="author-block">Long-term AI</div>

  <div class="eql-cntrb"><small><sup>*</sup>Indicates Equal Contribution</small></div>
</div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/sample.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Beijing-AISI/panda-guard" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.13862" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/datasets/Beijing-AISI/panda-bench" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg"
                        alt="Hugging Face" style="height: 1em; vertical-align: middle;">
                  </span>
                  <span>Huggingface</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://huggingface.co/spaces/Beijing-AISI/PandaGuard-leaderboard" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg"
                        alt="Hugging Face" style="height: 1em; vertical-align: middle;">
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>

              <span class="link-block">
              <a href="https://panda-guard.readthedocs.io/en/latest/" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-book"></i> <!-- Book icon from Font Awesome -->
                </span>
                <span>Documentation</span>
              </a>
            </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial prompts known as jailbreaks, which can bypass safety alignment and elicit harmful outputs. Despite growing efforts in LLM safety research, existing evaluations are often fragmented, focused on isolated attack or defense techniques, and lack systematic, reproducible analysis. In this work, we introduce PandaGuard, a unified and modular framework that models LLM jailbreak safety as a multi-agent system comprising attackers, defenders, and judges. Built on this framework, we develop PandaBench, a large-scale benchmark encompassing over 50 LLMs, 20+ attack methods, 10+ defense mechanisms, and multiple judgment strategies, requiring over 3 billion tokens to execute. Our comprehensive evaluation reveals key insights into model vulnerabilities, defense cost-performance trade-offs, and judge consistency. We find that no single defense is optimal across all dimensions and that judge disagreement introduces nontrivial variance in safety assessments. We release the full code, configurations, and evaluation results to support transparent and reproducible research in LLM safety.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <figure class="image">
          <img src="static/images/img-white.png" alt="Descriptive Alt Text">
        </figure>
        <p class="has-text-centered is-size-6 mt-2">The PandaGuard framework architecture illustrating the end-to-end pipeline for LLM safety evaluation. The system connects three key components: Attackers, Defenders, and Judges. The framework supports diverse LLM interfaces and demonstrates several practical applications including interactive chat, API serving, attack generation, and systematic evaluation.</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <figure class="image">
          <img src="static/images/models.png" alt="Descriptive Alt Text">
        </figure>
        <p class="has-text-centered is-size-6 mt-2">Model-wise safety analysis. (a) ASR vs. release date for various LLMs. (b) ASR across different harm categories with and without defense mechanisms. (c) Overall ASR for all evaluated LLMs with and without defense mechanisms.</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <figure class="image">
          <img src="static/images/asr.png" alt="Descriptive Alt Text">
        </figure>
        <p class="has-text-centered is-size-6 mt-2">ASR heatmap for different attack methods against various LLMs. Higher values indicate more successful attacks.</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <figure class="image">
          <img src="static/images/attack-defence.png" alt="Descriptive Alt Text">
        </figure>
        <p class="has-text-centered is-size-6 mt-2">Attack and defense mechanisms analysis. (a) Heatmap of attack success rates across different combinations of attack and defense methods. (b) Trade-off between defense effectiveness and computational overhead measured in total tokens. (c) Trade-off between defense effectiveness and impact on model performance as measured by Alpaca winrate.</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <figure class="image">
          <img src="static/images/pie.png" alt="Descriptive Alt Text">
        </figure>
        <p class="has-text-centered is-size-6 mt-2">Safety judge reliability analysis. (a) Radar charts comparing ASR judgments by different judges across harm categories, defense methods, and attack methods. Judges include rule-based and LLM-based (GPT-4o, Qwen2.5, Llama3.3). (b) Cohen's Kappa matrix showing agreement between different judges.</p>
      </div>
    </div>
  </div>
</section> -->








<!-- Introduction and Motivation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <h2 class="title is-3">Why PandaGuard Matters</h2>
      <p class="is-size-5">With the growing capabilities and adoption of large language models (LLMs), the risk of jailbreak attacks—where adversaries manipulate models into bypassing safety guardrails—has become a critical challenge. <strong>PandaGuard</strong> offers the most comprehensive framework for evaluating and enhancing the safety of LLMs across multiple models, attacks, and defenses.</p>
    </div>
    <!-- <div class="columns is-centered">
      <div class="column is-five-fifths has-text-centered">
        <figure class="image">
          <img src="static/images/img-white.png" alt="LLM Jailbreak Motivation">
        </figure>
        <p class="has-text-centered is-size-6 mt-2">Jailbreak attacks trick LLMs into generating harmful or unauthorized content. A systematic safety evaluation framework is urgently needed.</p>
      </div>
    </div> -->
  </div>
</section>

<!-- Framework Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <h2 class="title is-3">PandaGuard Framework</h2>
      <p class="is-size-5">PandaGuard structures LLM safety evaluation as an interactive system with <strong>Attackers</strong>, <strong>Defenders</strong>, and <strong>Judges</strong>. It supports plug-and-play of various algorithms, backends, and interfaces.</p>
    </div>
    <div class="columns is-centered">
      <div class="column is-five-fifths has-text-centered">
        <figure class="image">
          <img src="static/images/img-white.png" alt="PandaGuard Architecture">
        </figure>
        <p class="has-text-centered is-size-6 mt-2">The PandaGuard framework architecture illustrating the end-to-end pipeline for LLM safety evaluation. The system connects three key components: Attackers, Defenders, and Judges. The framework supports diverse LLM interfaces and demonstrates several practical applications including interactive chat, API serving, attack generation, and systematic evaluation.</p>
      </div>
    </div>
  </div>
</section>

<!-- Key Components -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
      <h2 class="title is-3 has-text-centered">System Components</h2>
      <div class="columns is-multiline">
        <div class="column is-half">
          <h4 class="title is-4">Attackers</h4>
          <p class="is-size-5">Includes 18+ prompt-based and instruction-tuning jailbreak attack strategies designed to elicit unauthorized outputs from the LLMs.</p>
        </div>
        <div class="column is-half">
          <h4 class="title is-4">Defenders</h4>
          <p class="is-size-5">Applies safety-alignment techniques such as reinforcement learning from human feedback (RLHF), paraphrasing, and response filtering.</p>
        </div>
        <div class="column is-half">
          <h4 class="title is-4">Judges</h4>
          <p class="is-size-5">Comprises rule-based, heuristic, and LLM-based evaluation strategies to assess jailbreak success rates and safety compliance.</p>
        </div>
        <div class="column is-half">
          <h4 class="title is-4">Multi-Backend Support</h4>
          <p class="is-size-5">Seamlessly integrates with inference engines such as <code>vLLM</code>, <code>SGLang</code>, and <code>Ollama</code> for scalable deployment and testing.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Experimental Insights -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <h2 class="title is-3">Empirical Results</h2>
      <p class="is-size-5">PandaBench, the evaluation suite of PandaGuard, covers over <strong>50+ LLMs</strong>, testing across <strong>multiple harm categories</strong> and defense setups, validated with more than <strong>1.5 billion tokens</strong>.</p>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <figure class="image">
          <img src="static/images/models.png" alt="aaa">
        </figure>
        <p class="has-text-centered is-size-6 mt-2">Model-wise safety analysis. (a) ASR vs. release date for various LLMs. (b) ASR across different harm categories with and without defense mechanisms. (c) Overall ASR for all evaluated LLMs with and without defense mechanisms.</p>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <figure class="image">
          <img src="static/images/asr.png" alt="bbb">
        </figure>
        <p class="has-text-centered is-size-6 mt-2"> ASR heatmap for different attack methods against various LLMs. Higher values indicate more successful attacks.</p>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <figure class="image">
          <img src="static/images/attack-defence.png" alt="ccc">
        </figure>
        <p class="has-text-centered is-size-6 mt-2"> Attack and defense mechanisms analysis. (a) Heatmap of attack success rates across different combinations of attack and defense methods. (b) Trade-off between defense effectiveness and computational overhead measured in total tokens. (c) Trade-off between defense effectiveness and impact on model performance as measured by Alpaca winrate.</p>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <figure class="image">
          <img src="static/images/pie.png" alt="ddd">
        </figure>
        <p class="has-text-centered is-size-6 mt-2"> Safety judge reliability analysis. (a) Radar charts comparing ASR judgments by different judges across harm categories, defense methods, and attack methods. Judges include rule-based and LLM-based (GPT-4o, Qwen2.5, Llama3.3). (b) Cohen's Kappa matrix showing agreement between different judges.</p>
      </div>
    </div>


  </div>
</section>

<!-- Conclusion -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <h2 class="title is-3">Conclusion</h2>
      <p class="is-size-5">PandaGuard represents a leap forward in the systematic safety evaluation of large language models. By integrating diverse components, providing reproducible pipelines, and supporting extensible backends, it offers a valuable toolkit for researchers and practitioners alike to ensure the safe and trustworthy deployment of LLMs.</p>
    </div>
  </div>
</section>












<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Manuscript</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
